{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe34f84-ad8a-44cb-bdd9-85c3e897b52c",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8bc9b26-bdb5-4447-bf86-12d43765e72c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 15:22:50.366519: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from pycox.datasets import metabric\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models import DeepHitSingle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dense, LSTM, Activation, Masking, Dropout\n",
    "#GW from keras.layers.normalization import BatchNormalization\n",
    "#GW from keras.callbacks.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#GW\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21882eca-538e-4bc5-8253-5c7d91f8e12e",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae59aaaf-bcca-4ad3-b8ac-cef938ec7597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x0        x1        x2        x3   x4   x5   x6   x7        x8  \\\n",
      "0    -0.703380  1.542005  0.024034  0.277774  1.0  1.0  0.0  1.0 -0.327232   \n",
      "1    -1.072979  3.275476 -0.413114 -0.609064  1.0  0.0  0.0  1.0  1.914901   \n",
      "2    -0.336718  0.528334  1.227642  0.003052  0.0  1.0  0.0  1.0 -0.974446   \n",
      "3     0.513564 -0.877053 -1.561109 -0.635679  0.0  0.0  0.0  0.0  0.448654   \n",
      "4    -0.873823 -0.879114 -0.154447  0.397396  1.0  0.0  0.0  1.0  0.521080   \n",
      "...        ...       ...       ...       ...  ...  ...  ...  ...       ...   \n",
      "1899 -0.305737 -0.848992  1.164331 -0.385112  1.0  1.0  0.0  1.0  1.213752   \n",
      "1900 -1.010003 -0.811416  1.039324 -0.526802  1.0  1.0  0.0  1.0  0.154325   \n",
      "1901 -0.358319 -0.945238  2.531080  0.781158  0.0  0.0  0.0  1.0 -0.255577   \n",
      "1902  0.703712 -0.846786  0.653666  0.600831  1.0  0.0  0.0  1.0 -0.169282   \n",
      "1903 -0.562153 -0.771385 -0.799073  2.119062  1.0  1.0  0.0  0.0 -0.035216   \n",
      "\n",
      "            time  status  \n",
      "0      99.333336       0  \n",
      "1      95.733330       1  \n",
      "2     140.233340       0  \n",
      "3     239.300000       0  \n",
      "4      56.933334       1  \n",
      "...          ...     ...  \n",
      "1899   87.233330       1  \n",
      "1900  157.533340       0  \n",
      "1901   37.866665       1  \n",
      "1902  198.433330       0  \n",
      "1903  140.766660       0  \n",
      "\n",
      "[1904 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df = metabric.read_df() # read in dataset\n",
    "\n",
    "df = pd.read_csv(\"datasets/metabric/full.csv\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8625191c-a4d0-42d2-86d0-8e40613ef89f",
   "metadata": {},
   "source": [
    "# Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2eddfbd-22b3-44ba-b9b9-df654ac98171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalise(df, colnames):\n",
    "    df[colnames] = df[colnames].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefdd8c-94c3-486f-8afd-4f5f9e7d5ac7",
   "metadata": {},
   "source": [
    "# Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa94d4bf-06fb-4174-a458-e8b6d636e4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_train_test(df, train_frac, dataset, n_splits=3):\n",
    "\n",
    "    os.makedirs( \"datasets/\" + dataset, exist_ok=True)\n",
    " \n",
    "    full_file_path = \"datasets/\" + dataset + \"/full.csv\"\n",
    "    df.to_csv(full_file_path, index=False)\n",
    "    \n",
    "    for i in range(1, n_splits+1):\n",
    "\n",
    "        # set seed equal to loop index\n",
    "        random.seed(123*i)\n",
    "\n",
    "        # set output file paths\n",
    "        train_file_path = \"datasets/\" + dataset + \"/train_\" + str(i) + \".csv\"\n",
    "        test_file_path = \"datasets/\" + dataset + \"/test_\" + str(i) + \".csv\"\n",
    "\n",
    "        # create splits (different each time - sample depends on seed)\n",
    "        train_df = df.groupby(\"status\").apply(lambda x: x.sample(frac=train_frac)) # censoring frac. equal in train and test sets\n",
    "        train_df = train_df.reset_index(level=\"status\", drop=True)\n",
    "        train_df = train_df.sort_index()\n",
    "        test_df = df.drop(train_df.index)\n",
    "\n",
    "        # save the resulting file\n",
    "        train_df.to_csv(train_file_path, index=False)\n",
    "        test_df.to_csv(test_file_path, index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbfab65-eed6-41bb-a6fc-02884e291af0",
   "metadata": {},
   "source": [
    "# define the models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b43566-1e13-4e2f-89a0-f8de0cfe2528",
   "metadata": {},
   "source": [
    "# define regression weibull\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ccda27-112c-4a52-b77c-fbf19fd9d11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def weibull_surv(t, alpha, beta):\n",
    "\n",
    "    S = np.empty((len(t),len(alpha)))\n",
    "    \n",
    "    for i in range(len(alpha)):\n",
    "        S[:,i] = np.exp(-np.power(np.divide(t, alpha[i]), beta[i]))\n",
    "\n",
    "    return S\n",
    "\n",
    "def regression_gumbel_loglkhd(theta, sigma, df):\n",
    "    \n",
    "    x = df.drop([\"time\", \"status\"], axis=1) # Pandas dataframe of covariate columns\n",
    "    mu = theta[0] + x.dot(np.array(theta[1:])) # mu = theta_0 + < theta[1:p], x >\n",
    "     \n",
    "    l = np.sum(df[\"status\"] * (np.divide(np.subtract(np.log(df[\"time\"]),mu),sigma))-np.log(sigma)) # first sum of l\n",
    "    l -= np.sum(np.exp(np.divide(np.subtract(np.log(df[\"time\"]),mu),sigma))) # second sum of l\n",
    "    \n",
    "    return l\n",
    "\n",
    "def regression_weibull(dataset, split):\n",
    "\n",
    "    \"\"\"\n",
    "    Paths to input and output files\n",
    "    \"\"\"\n",
    "    train_path = \"datasets/\" + dataset + \"/train_\" + str(split) + \".csv\" # training set data\n",
    "    test_path = \"datasets/\" + dataset + \"/test_\" + str(split) + \".csv\" # test set data\n",
    "\n",
    "    \"\"\"\n",
    "    Read in the appropriate training and test sets (dataset name and split index)\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # intialise parameters for the model\n",
    "    p = train_df.shape[1] - 2 # number of covariates (number of columns excluding time, status)\n",
    "    median_log_time = np.log(train_df[\"time\"]).median() # compute median time for estimate of alpha\n",
    "    init_theta = [median_log_time] + [0]*p # [*,0,...,0] # for mu=median log time\n",
    "    init_sigma = [1]\n",
    "    init_params = init_theta + init_sigma\n",
    "\n",
    "    # fit the model\n",
    "    fun = lambda x: -1 * regression_gumbel_loglkhd(x[:-1], x[-1], train_df) # wrapper for negative log-lkhd function\n",
    "    res = scipy.optimize.minimize(fun, x0=init_params) # minimise negative log-lkhd\n",
    "\n",
    "    # make predictions on the test set\n",
    "    parameters = res.x\n",
    "    theta = parameters[:-1]\n",
    "    sigma = parameters[-1]\n",
    "    test_result = test_df.copy()\n",
    "    x = test_result.drop([\"time\", \"status\"], axis=1)\n",
    "    test_result[\"pred_alpha\"] = np.exp(theta[0] + x.dot(np.array(theta[1:]))) # alpha = exp(mu)\n",
    "    test_result[\"pred_beta\"] = 1/sigma # beta = 1/sigma\n",
    "\n",
    "    # if any of the predicted alpha/beta are negative, set equal to small eps\n",
    "    eps = 1e-8\n",
    "    test_result[\"pred_alpha\"] = np.maximum(test_result[\"pred_alpha\"], eps*np.ones(len(test_result[\"pred_alpha\"])))\n",
    "    test_result[\"pred_beta\"] = np.maximum(test_result[\"pred_beta\"], eps*np.ones(len(test_result[\"pred_beta\"])))\n",
    "\n",
    "    \"\"\"\n",
    "    Create EvalSurv object\n",
    "    \"\"\"\n",
    "    t_max = train_df[\"time\"].max()\n",
    "    num_vals = max(math.ceil(t_max), 5000)\n",
    "    t_vals = np.linspace(0, t_max, num_vals)\n",
    "    surv = weibull_surv(t_vals, test_result[\"pred_alpha\"], test_result[\"pred_beta\"])\n",
    "    surv = pd.DataFrame(data=surv, index=t_vals)\n",
    "\n",
    "    test_time = test_df['time'].values\n",
    "    test_status = test_df['status'].values\n",
    "\n",
    "    ev = EvalSurv(surv, test_time, test_status, censor_surv='km')\n",
    "    \n",
    "    return ({\"parameters\" : parameters, \"test_result\" : test_result, \"ev\" : ev }) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00cacd-62b9-4930-92d7-83313d00dc18",
   "metadata": {},
   "source": [
    "# define deep hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d06b363d-fd01-4c24-9925-c263163d65cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deep_hit(dataset, split, plot=False, alpha=0.5, lr=1e-4, epochs=50, batch_size=100):\n",
    "\n",
    "    \"\"\"\n",
    "    Paths to input and output files\n",
    "    \"\"\"\n",
    "    train_path = \"datasets/\" + dataset + \"/train_\" + str(split) + \".csv\" # training set data\n",
    "    test_path = \"datasets/\" + dataset + \"/test_\" + str(split) + \".csv\" # test set data\n",
    " \n",
    "    training_loss_plot_path = \"plots/deep_hit/training_loss/\" + dataset + \"_\" + str(split) + \".pdf\" # create plot of loss for different lr's\n",
    "\n",
    "    \"\"\"\n",
    "    Read in the appropriate training and test sets (dataset name and split index)\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    \"\"\"\n",
    "    Data preprocessing: set up for DeepHitSingle\n",
    "    \"\"\"\n",
    "\n",
    "    # split the training set into training and validation set\n",
    "    df = train_df.copy()\n",
    "    train_df = df.groupby(\"status\").apply(lambda x: x.sample(frac=0.8)) # censoring frac. equal in train and test sets\n",
    "    train_df = train_df.reset_index(level=\"status\", drop=True)\n",
    "    train_df = train_df.sort_index()\n",
    "    val_df = df.drop(train_df.index)\n",
    "\n",
    "    # convert x values to float32 (needed for PyTorch)\n",
    "    x_cols = list(train_df)\n",
    "    x_cols.remove('time')\n",
    "    x_cols.remove('status')\n",
    "    x_cols = [(col, None) for col in x_cols]\n",
    "    x_mapper = DataFrameMapper(x_cols)\n",
    "    train_x = x_mapper.fit_transform(train_df).astype('float32')\n",
    "    val_x = x_mapper.transform(val_df).astype('float32')\n",
    "    test_x = x_mapper.transform(test_df).astype('float32')\n",
    "\n",
    "    # discretise time for DeepHit, using time index set {0,1,...,T_max}\n",
    "    num_durations = math.ceil(train_df[\"time\"].max()) # largest survival time in training set set to be T_max \n",
    "    labtrans = DeepHitSingle.label_transform(num_durations, scheme='equidistant') # set up partition\n",
    "    get_target = lambda df: (df['time'].values, df['status'].values) \n",
    "    train_y = labtrans.fit_transform(*get_target(train_df))\n",
    "    val_y = labtrans.transform(*get_target(val_df))\n",
    "    test_time, test_status = get_target(test_df)\n",
    "\n",
    "    \"\"\"\n",
    "    Define the DeepHit network. Since K=1 (single event) the architecture is very simple.\n",
    "    Model architecture: layers and widths as stated in paper; use dropout probability 0.1 and batch normalisation \n",
    "    \"\"\"\n",
    "\n",
    "    p = train_x.shape[1] # number of covariates\n",
    "    in_features = p # number of input nodes = number of covariates\n",
    "    out_features = labtrans.out_features # equals num_durations \n",
    "    nodes_1 = 3*p\n",
    "    nodes_2 = 5*p\n",
    "    nodes_3 = 3*p\n",
    "    num_nodes = [nodes_1,nodes_2,nodes_3] # layer widths as stated in DeepHit paper\n",
    "    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm=False, dropout=0.25)\n",
    "\n",
    "    \"\"\"\n",
    "    Set learning parameters and fit the model.\n",
    "    NB: alpha in DeepHitSingle() differs from DH paper: alpha_{pycox} = 1/(1+alpha_{DH}\n",
    "    The deep_hit(..., alpha, ...) refers to alpha as in the Deep Hit paper.\n",
    "    \"\"\"\n",
    "\n",
    "    alpha_pycox = 1/(1+alpha)\n",
    "    model = DeepHitSingle(net, tt.optim.Adam, alpha=alpha_pycox, sigma=0.2, duration_index=labtrans.cuts)\n",
    "\n",
    "    \"\"\"\n",
    "    Train the model (with early stopping) and plot the training and validation loss.\n",
    "    \"\"\"\n",
    "    model.optimizer.set_lr(lr)\n",
    "    callbacks = [tt.callbacks.EarlyStopping()]\n",
    "    log = model.fit(train_x, train_y, batch_size, epochs, callbacks, val_data=(val_x, val_y))\n",
    "\n",
    "    if plot ==True: \n",
    "      log.plot()\n",
    "      plt.xlabel(\"Epoch\")\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.title(\"Training loss: DeepHit ($\\\\alpha =$\" + str(alpha) + \") on \" + tidy_datasets[dataset] + \" (Split \" + str(split) + \")\")\n",
    "      plt.legend(['Train', 'Validation'])\n",
    "      plt.savefig(training_loss_plot_path)\n",
    "      plt.clf()\n",
    "      plt.close('all')\n",
    "    \n",
    "    \"\"\"\n",
    "    Predict the survival curves for the test set \n",
    "    \"\"\"\n",
    "\n",
    "    surv = model.predict_surv_df(test_x)\n",
    "\n",
    "    \"\"\"\n",
    "    Create evaluation object\n",
    "    \"\"\"\n",
    "\n",
    "    ev = EvalSurv(surv, test_time, test_status, censor_surv='km')\n",
    "\n",
    "    return ({\"test_result\" : surv, \"ev\" : ev})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cea48-7447-4fa9-beba-8bb80f77741d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# define deep weibull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6b7536-ecca-4338-954f-4ce9560a42ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "CUSTOM LOSS AND ACTIVATION FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def weibull_surv(t, alpha, beta):\n",
    "\n",
    "    S = np.empty((len(t),len(alpha)))\n",
    "    \n",
    "    for i in range(len(alpha)):\n",
    "        S[:,i] = np.exp(-np.power(np.divide(t, alpha[i]), beta[i]))\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "def deep_weibull_loss(y, weibull_param_pred, name=None):\n",
    "    epsilon = 1e-10\n",
    "    time = y[:, 0] # actual time to event\n",
    "    status = y[:, 1] # actual status (censored/dead)\n",
    "    a = weibull_param_pred[:, 0] # alpha\n",
    "    b = weibull_param_pred[:, 1] # beta\n",
    "    norm_time = (time + epsilon) / a # time / alpha (rescaled time)\n",
    "    return -1 * k.mean(status * (k.log(b) + b * k.log(norm_time)) - k.pow(norm_time, b))\n",
    "\n",
    "\n",
    "def weibull_activate(weibull_param):\n",
    "    a = k.exp(weibull_param[:, 0]) # exponential of alpha \n",
    "    #a = k.softplus(weibull_param[:, 0]) # softplus of alpha\n",
    "    b = k.softplus(weibull_param[:, 1]) # softplus of beta\n",
    "    a = k.reshape(a, (k.shape(a)[0], 1))\n",
    "    b = k.reshape(b, (k.shape(b)[0], 1))\n",
    "    return k.concatenate((a, b), axis=1)\n",
    "\n",
    "def deep_weibull(dataset, split, plot=False, lr=1e-4, epochs=75, steps_per_epoch=25):\n",
    "\n",
    "    \"\"\"\n",
    "    Paths to input and output files\n",
    "    \"\"\"\n",
    "    train_path = \"datasets/\" + dataset + \"/train_\" + str(split) + \".csv\" # training set data\n",
    "    test_path = \"datasets/\" + dataset + \"/test_\" + str(split) + \".csv\" # test set data\n",
    " \n",
    "    training_loss_plot_path = \"plots/deep_weibull/training_loss/\" + dataset + \"_\" + str(split) + \".pdf\" # create plot of loss for different lr's\n",
    "\n",
    "    \"\"\"\n",
    "    Read in the appropriate training and test sets (dataset name and split index)\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    \"\"\"\n",
    "    Data preprocessing: set up for DeepWeibull\n",
    "    \"\"\"\n",
    "\n",
    "    # split the training set into training and validation set\n",
    "    df = train_df.copy()\n",
    "    train_df = df.groupby(\"status\").apply(lambda x: x.sample(frac=0.8)) # censoring frac. equal in train and test sets\n",
    "    train_df = train_df.reset_index(level=\"status\", drop=True)\n",
    "    train_df = train_df.sort_index()\n",
    "    val_df = df.drop(train_df.index)\n",
    "\n",
    "    # separate covariates and outcomes\n",
    "    train_x = train_df.copy()\n",
    "    test_x = test_df.copy()\n",
    "    val_x = val_df.copy()\n",
    "    train_y = pd.DataFrame([train_x.pop(colname) for colname in ['time', 'status']]).T\n",
    "    test_y = pd.DataFrame([test_x.pop(colname) for colname in ['time', 'status']]).T\n",
    "    val_y = pd.DataFrame([val_x.pop(colname) for colname in ['time', 'status']]).T\n",
    "\n",
    "    # convert to tensors and float32 type\n",
    "    train_x = tf.convert_to_tensor(train_x.values, tf.float32)\n",
    "    train_y = tf.convert_to_tensor(train_y.values, tf.float32)\n",
    "    test_x = tf.convert_to_tensor(test_x.values, tf.float32)\n",
    "    test_y = tf.convert_to_tensor(test_y.values, tf.float32)\n",
    "    val_x = tf.convert_to_tensor(val_x.values, tf.float32)\n",
    "    val_y = tf.convert_to_tensor(val_y.values, tf.float32)\n",
    "\n",
    "    \"\"\"\n",
    "    Define the DeepWeibull network. \n",
    "    Model architecture: layers and widths as stated in paper; use dropout probability 0.1 and batch normalisation \n",
    "    'glorot_normal' is Xavier initialisation\n",
    "    \"\"\"\n",
    "\n",
    "    p = train_x.shape[1] # number of covariates\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(p, input_dim=p, activation='tanh', kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(2*p, activation='tanh', kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(p, activation='tanh', kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(2)) # layer with 2 nodes (alpha and beta)\n",
    "    model.add(Activation(weibull_activate)) # apply custom activation function (exp and softplus)\n",
    "\n",
    "    \"\"\"\n",
    "    Compile the model:\n",
    "        - using the (negative) log-likelihood for the Weibull as the loss function\n",
    "        - using Root Mean Square Prop optimisation (common) and customisable learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    model.compile(loss=deep_weibull_loss, optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and plot the training and validation loss.\n",
    "    \"\"\"\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)\n",
    "    callbacks = [es] # use [EarlyStopping()] if desired\n",
    "    log = model.fit(train_x, train_y, \n",
    "        epochs=epochs, \n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        validation_data=(val_x, val_y), \n",
    "        callbacks=callbacks, \n",
    "        validation_steps=5, \n",
    "        verbose=1)\n",
    "\n",
    "    if plot==True:\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title('Training loss: DeepWeibull on '+ tidy_datasets[dataset] + \" (Split \" + str(split) + \")\")\n",
    "        plt.legend(['Train', 'Validation'])\n",
    "        plt.savefig(training_loss_plot_path)\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "\n",
    "    \"\"\"\n",
    "    Use learnt model to make predictions on the test set\n",
    "    \"\"\"\n",
    "\n",
    "    test_predict = model.predict(test_x, steps=1) # predict Weibull parameters using covariates\n",
    "    test_predict = np.resize(test_predict, test_y.shape) # put into (,2) array\n",
    "    test_predict = pd.DataFrame(test_predict) # convert to dataframe\n",
    "    test_predict.columns = [\"pred_alpha\", \"pred_beta\"] # name columns\n",
    "    test_result = test_df.copy()\n",
    "    test_result.reset_index(inplace = True) # reset the index (before concat - probably better way of doing this)\n",
    "    test_result = pd.concat([test_result, test_predict], axis=1) # results = test data plus predictions\n",
    "    test_result.set_index(\"index\", drop=True, inplace=True) # recover the index (after concat - probably better way of doing this)\n",
    "\n",
    "    \"\"\"\n",
    "    Create EvalSurv object\n",
    "    \"\"\"\n",
    "    t_max = train_df[\"time\"].max()\n",
    "    num_vals = max(math.ceil(t_max), 5000)\n",
    "    t_vals = np.linspace(0, t_max, num_vals)\n",
    "    surv = weibull_surv(t_vals, test_result[\"pred_alpha\"], test_result[\"pred_beta\"])\n",
    "    surv = pd.DataFrame(data=surv, index=t_vals)\n",
    "\n",
    "    test_time = test_df['time'].values\n",
    "    test_status = test_df['status'].values\n",
    "\n",
    "    ev = EvalSurv(surv, test_time, test_status, censor_surv='km')\n",
    "\n",
    "    return ({\"test_result\" : test_result, \"ev\" : ev})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed543e1-909d-43a5-a3d8-fd7e597c6e82",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea1ed44-2aa6-402a-9ae4-d61b118956ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to train <function regression_weibull at 0x7f893657a200>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to train <function regression_weibull at 0x7f893657a200>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_vals [0.630631900635878, 0.6549077468777013]\n",
      "about to train <function deep_hit at 0x7f8936033830>\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 2.6592,\tval_loss: 2.6199\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 2.7126,\tval_loss: 2.6181\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 2.6054,\tval_loss: 2.6164\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 2.7093,\tval_loss: 2.6149\n",
      "4:\t[0s / 1s],\t\ttrain_loss: 2.6602,\tval_loss: 2.6134\n",
      "5:\t[0s / 1s],\t\ttrain_loss: 2.7020,\tval_loss: 2.6121\n",
      "6:\t[0s / 1s],\t\ttrain_loss: 2.6556,\tval_loss: 2.6108\n",
      "7:\t[0s / 1s],\t\ttrain_loss: 2.6592,\tval_loss: 2.6095\n",
      "8:\t[0s / 2s],\t\ttrain_loss: 2.6868,\tval_loss: 2.6084\n",
      "9:\t[0s / 2s],\t\ttrain_loss: 2.6512,\tval_loss: 2.6073\n",
      "10:\t[0s / 2s],\t\ttrain_loss: 2.6918,\tval_loss: 2.6064\n",
      "11:\t[0s / 2s],\t\ttrain_loss: 2.6502,\tval_loss: 2.6057\n",
      "12:\t[0s / 2s],\t\ttrain_loss: 2.6663,\tval_loss: 2.6049\n",
      "13:\t[0s / 2s],\t\ttrain_loss: 2.6447,\tval_loss: 2.6042\n",
      "14:\t[0s / 3s],\t\ttrain_loss: 2.6168,\tval_loss: 2.6034\n",
      "15:\t[0s / 3s],\t\ttrain_loss: 2.6165,\tval_loss: 2.6029\n",
      "16:\t[0s / 3s],\t\ttrain_loss: 2.6977,\tval_loss: 2.6023\n",
      "17:\t[0s / 3s],\t\ttrain_loss: 2.6603,\tval_loss: 2.6017\n",
      "18:\t[0s / 3s],\t\ttrain_loss: 2.5781,\tval_loss: 2.6011\n",
      "19:\t[0s / 3s],\t\ttrain_loss: 2.6650,\tval_loss: 2.6006\n",
      "20:\t[0s / 4s],\t\ttrain_loss: 2.6843,\tval_loss: 2.5999\n",
      "21:\t[0s / 4s],\t\ttrain_loss: 2.5420,\tval_loss: 2.5994\n",
      "22:\t[0s / 4s],\t\ttrain_loss: 2.5946,\tval_loss: 2.5990\n",
      "23:\t[0s / 4s],\t\ttrain_loss: 2.6000,\tval_loss: 2.5985\n",
      "24:\t[0s / 4s],\t\ttrain_loss: 2.5940,\tval_loss: 2.5982\n",
      "25:\t[0s / 4s],\t\ttrain_loss: 2.6355,\tval_loss: 2.5979\n",
      "26:\t[0s / 5s],\t\ttrain_loss: 2.5584,\tval_loss: 2.5975\n",
      "27:\t[0s / 5s],\t\ttrain_loss: 2.5055,\tval_loss: 2.5971\n",
      "28:\t[0s / 5s],\t\ttrain_loss: 2.6255,\tval_loss: 2.5969\n",
      "29:\t[0s / 5s],\t\ttrain_loss: 2.6382,\tval_loss: 2.5966\n",
      "30:\t[0s / 5s],\t\ttrain_loss: 2.6293,\tval_loss: 2.5963\n",
      "31:\t[0s / 5s],\t\ttrain_loss: 2.5991,\tval_loss: 2.5960\n",
      "32:\t[0s / 6s],\t\ttrain_loss: 2.6236,\tval_loss: 2.5958\n",
      "33:\t[0s / 6s],\t\ttrain_loss: 2.6370,\tval_loss: 2.5955\n",
      "34:\t[0s / 6s],\t\ttrain_loss: 2.5341,\tval_loss: 2.5952\n",
      "35:\t[0s / 6s],\t\ttrain_loss: 2.6468,\tval_loss: 2.5950\n",
      "36:\t[0s / 6s],\t\ttrain_loss: 2.5860,\tval_loss: 2.5948\n",
      "37:\t[0s / 7s],\t\ttrain_loss: 2.5822,\tval_loss: 2.5946\n",
      "38:\t[0s / 7s],\t\ttrain_loss: 2.6453,\tval_loss: 2.5945\n",
      "39:\t[0s / 7s],\t\ttrain_loss: 2.5603,\tval_loss: 2.5943\n",
      "40:\t[0s / 7s],\t\ttrain_loss: 2.5159,\tval_loss: 2.5941\n",
      "41:\t[0s / 7s],\t\ttrain_loss: 2.5881,\tval_loss: 2.5939\n",
      "42:\t[0s / 7s],\t\ttrain_loss: 2.5511,\tval_loss: 2.5937\n",
      "43:\t[0s / 7s],\t\ttrain_loss: 2.5496,\tval_loss: 2.5936\n",
      "44:\t[0s / 7s],\t\ttrain_loss: 2.6211,\tval_loss: 2.5935\n",
      "45:\t[0s / 8s],\t\ttrain_loss: 2.5866,\tval_loss: 2.5931\n",
      "46:\t[0s / 8s],\t\ttrain_loss: 2.6073,\tval_loss: 2.5930\n",
      "47:\t[0s / 8s],\t\ttrain_loss: 2.4790,\tval_loss: 2.5928\n",
      "48:\t[0s / 8s],\t\ttrain_loss: 2.6018,\tval_loss: 2.5927\n",
      "49:\t[0s / 9s],\t\ttrain_loss: 2.5566,\tval_loss: 2.5925\n",
      "about to train <function deep_hit at 0x7f8936033830>\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 2.6739,\tval_loss: 2.6069\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 2.7382,\tval_loss: 2.6047\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 2.6663,\tval_loss: 2.6024\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 2.6850,\tval_loss: 2.6003\n",
      "4:\t[0s / 1s],\t\ttrain_loss: 2.6569,\tval_loss: 2.5985\n",
      "5:\t[0s / 1s],\t\ttrain_loss: 2.6716,\tval_loss: 2.5968\n",
      "6:\t[0s / 1s],\t\ttrain_loss: 2.6470,\tval_loss: 2.5952\n",
      "7:\t[0s / 1s],\t\ttrain_loss: 2.6366,\tval_loss: 2.5938\n",
      "8:\t[0s / 1s],\t\ttrain_loss: 2.5826,\tval_loss: 2.5922\n",
      "9:\t[0s / 1s],\t\ttrain_loss: 2.7233,\tval_loss: 2.5908\n",
      "10:\t[0s / 1s],\t\ttrain_loss: 2.5868,\tval_loss: 2.5892\n",
      "11:\t[0s / 2s],\t\ttrain_loss: 2.6562,\tval_loss: 2.5878\n",
      "12:\t[0s / 2s],\t\ttrain_loss: 2.6408,\tval_loss: 2.5868\n",
      "13:\t[0s / 2s],\t\ttrain_loss: 2.7094,\tval_loss: 2.5856\n",
      "14:\t[0s / 2s],\t\ttrain_loss: 2.6661,\tval_loss: 2.5845\n",
      "15:\t[0s / 2s],\t\ttrain_loss: 2.5931,\tval_loss: 2.5834\n",
      "16:\t[0s / 2s],\t\ttrain_loss: 2.6782,\tval_loss: 2.5824\n",
      "17:\t[0s / 3s],\t\ttrain_loss: 2.6948,\tval_loss: 2.5815\n",
      "18:\t[0s / 3s],\t\ttrain_loss: 2.6175,\tval_loss: 2.5807\n",
      "19:\t[0s / 3s],\t\ttrain_loss: 2.5993,\tval_loss: 2.5800\n",
      "20:\t[0s / 3s],\t\ttrain_loss: 2.6224,\tval_loss: 2.5793\n",
      "21:\t[0s / 3s],\t\ttrain_loss: 2.6279,\tval_loss: 2.5784\n",
      "22:\t[0s / 3s],\t\ttrain_loss: 2.6887,\tval_loss: 2.5777\n",
      "23:\t[0s / 3s],\t\ttrain_loss: 2.6344,\tval_loss: 2.5770\n",
      "24:\t[0s / 3s],\t\ttrain_loss: 2.6156,\tval_loss: 2.5763\n",
      "25:\t[0s / 4s],\t\ttrain_loss: 2.5668,\tval_loss: 2.5756\n",
      "26:\t[0s / 4s],\t\ttrain_loss: 2.5554,\tval_loss: 2.5750\n",
      "27:\t[0s / 4s],\t\ttrain_loss: 2.5741,\tval_loss: 2.5743\n",
      "28:\t[0s / 4s],\t\ttrain_loss: 2.6266,\tval_loss: 2.5736\n",
      "29:\t[0s / 4s],\t\ttrain_loss: 2.5460,\tval_loss: 2.5730\n",
      "30:\t[0s / 4s],\t\ttrain_loss: 2.5523,\tval_loss: 2.5725\n",
      "31:\t[0s / 4s],\t\ttrain_loss: 2.5500,\tval_loss: 2.5720\n",
      "32:\t[0s / 4s],\t\ttrain_loss: 2.6797,\tval_loss: 2.5714\n",
      "33:\t[0s / 5s],\t\ttrain_loss: 2.5884,\tval_loss: 2.5710\n",
      "34:\t[0s / 5s],\t\ttrain_loss: 2.6188,\tval_loss: 2.5706\n",
      "35:\t[0s / 5s],\t\ttrain_loss: 2.5603,\tval_loss: 2.5701\n",
      "36:\t[0s / 5s],\t\ttrain_loss: 2.5356,\tval_loss: 2.5696\n",
      "37:\t[0s / 5s],\t\ttrain_loss: 2.5873,\tval_loss: 2.5692\n",
      "38:\t[0s / 5s],\t\ttrain_loss: 2.5807,\tval_loss: 2.5687\n",
      "39:\t[0s / 6s],\t\ttrain_loss: 2.5700,\tval_loss: 2.5683\n",
      "40:\t[0s / 6s],\t\ttrain_loss: 2.5174,\tval_loss: 2.5679\n",
      "41:\t[0s / 6s],\t\ttrain_loss: 2.6077,\tval_loss: 2.5674\n",
      "42:\t[0s / 6s],\t\ttrain_loss: 2.5751,\tval_loss: 2.5670\n",
      "43:\t[0s / 6s],\t\ttrain_loss: 2.5585,\tval_loss: 2.5667\n",
      "44:\t[0s / 6s],\t\ttrain_loss: 2.5664,\tval_loss: 2.5663\n",
      "45:\t[0s / 7s],\t\ttrain_loss: 2.5568,\tval_loss: 2.5658\n",
      "46:\t[0s / 7s],\t\ttrain_loss: 2.5733,\tval_loss: 2.5652\n",
      "47:\t[0s / 7s],\t\ttrain_loss: 2.6106,\tval_loss: 2.5648\n",
      "48:\t[0s / 7s],\t\ttrain_loss: 2.5439,\tval_loss: 2.5644\n",
      "49:\t[0s / 7s],\t\ttrain_loss: 2.5102,\tval_loss: 2.5641\n",
      "c_vals [0.5445542560529244, 0.4860452869931543]\n",
      "about to train <function deep_weibull at 0x7f893657ad40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 15:23:31.150789: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 2s 18ms/step - loss: 133.1396 - val_loss: 22.0866\n",
      "Epoch 2/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 109.2937 - val_loss: 18.7555\n",
      "Epoch 3/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 87.3001 - val_loss: 16.3341\n",
      "Epoch 4/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 108.7979 - val_loss: 13.9349\n",
      "Epoch 5/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 50.7238 - val_loss: 12.6846\n",
      "Epoch 6/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 60.7845 - val_loss: 11.2891\n",
      "Epoch 7/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 61.8801 - val_loss: 10.3998\n",
      "Epoch 8/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 43.1837 - val_loss: 9.3205\n",
      "Epoch 9/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 786.1989 - val_loss: 8.5187\n",
      "Epoch 10/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 28.0007 - val_loss: 8.3070\n",
      "Epoch 11/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 30.3652 - val_loss: 7.9821\n",
      "Epoch 12/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 37.1714 - val_loss: 7.6894\n",
      "Epoch 13/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 26.4449 - val_loss: 7.3348\n",
      "Epoch 14/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 27.3486 - val_loss: 6.7703\n",
      "Epoch 15/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 22.6259 - val_loss: 6.3416\n",
      "Epoch 16/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 16.6651 - val_loss: 5.8332\n",
      "Epoch 17/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 16.9485 - val_loss: 5.3735\n",
      "Epoch 18/75\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 18.0096 - val_loss: 5.1039\n",
      "Epoch 19/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 10.7916 - val_loss: 4.8077\n",
      "Epoch 20/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 11.0082 - val_loss: 4.4766\n",
      "Epoch 21/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 10.1997 - val_loss: 4.2016\n",
      "Epoch 22/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 8.5416 - val_loss: 3.9985\n",
      "Epoch 23/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 8.1078 - val_loss: 3.7906\n",
      "Epoch 24/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.8229 - val_loss: 3.6026\n",
      "Epoch 25/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 6.1497 - val_loss: 3.4653\n",
      "Epoch 26/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 5.6209 - val_loss: 3.3202\n",
      "Epoch 27/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 5.1975 - val_loss: 3.1978\n",
      "Epoch 28/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 5.4014 - val_loss: 3.1035\n",
      "Epoch 29/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 4.6336 - val_loss: 3.0254\n",
      "Epoch 30/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 5.3852 - val_loss: 2.9719\n",
      "Epoch 31/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 4.3252 - val_loss: 2.9214\n",
      "Epoch 32/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 4.2050 - val_loss: 2.8674\n",
      "Epoch 33/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 4.5194 - val_loss: 2.8244\n",
      "Epoch 34/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 4.6482 - val_loss: 2.7976\n",
      "Epoch 35/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 3.7703 - val_loss: 2.7780\n",
      "Epoch 36/75\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 3.6554 - val_loss: 2.7526\n",
      "Epoch 37/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.9156 - val_loss: 2.7330\n",
      "Epoch 38/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 3.9985 - val_loss: 2.7182\n",
      "Epoch 39/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 3.6328 - val_loss: 2.7083\n",
      "Epoch 40/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 3.4942 - val_loss: 2.6972\n",
      "Epoch 41/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 4.5588 - val_loss: 2.6895\n",
      "Epoch 42/75\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 3.3034 - val_loss: 2.6835\n",
      "Epoch 43/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 3.3619 - val_loss: 2.6771\n",
      "Epoch 44/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 3.2631 - val_loss: 2.6719\n",
      "Epoch 45/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 3.8418 - val_loss: 2.6690\n",
      "Epoch 46/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 3.2438 - val_loss: 2.6673\n",
      "Epoch 47/75\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 3.1935 - val_loss: 2.6659\n",
      "Epoch 48/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 3.0741 - val_loss: 2.6651\n",
      "Epoch 49/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.0085 - val_loss: 2.6658\n",
      "Epoch 50/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.0460 - val_loss: 2.6660\n",
      "Epoch 51/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 2.9042 - val_loss: 2.6662\n",
      "Epoch 52/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 2.9839 - val_loss: 2.6660\n",
      "Epoch 53/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.0103 - val_loss: 2.6679\n",
      "Epoch 54/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 2.9120 - val_loss: 2.6694\n",
      "Epoch 55/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 2.9016 - val_loss: 2.6707\n",
      "Epoch 56/75\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 2.9213 - val_loss: 2.6719\n",
      "Epoch 57/75\n",
      "25/25 [==============================] - 1s 15ms/step - loss: 2.9895 - val_loss: 2.6731\n",
      "Epoch 58/75\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 2.9665 - val_loss: 2.6744\n",
      "Epoch 59/75\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 2.8273 - val_loss: 2.6734\n",
      "Epoch 60/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 2.9038 - val_loss: 2.6744\n",
      "Epoch 61/75\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 2.8566 - val_loss: 2.6740\n",
      "Epoch 62/75\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 2.8096 - val_loss: 2.6716\n",
      "Epoch 63/75\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 2.8244 - val_loss: 2.6722\n",
      "Epoch 63: early stopping\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "about to train <function deep_weibull at 0x7f893657ad40>\n",
      "Epoch 1/75\n",
      "25/25 [==============================] - 2s 20ms/step - loss: 2345.0735 - val_loss: 532.3979\n",
      "Epoch 2/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 1492.7549 - val_loss: 478.3163\n",
      "Epoch 3/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 933.4050 - val_loss: 420.5084\n",
      "Epoch 4/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1094.9360 - val_loss: 363.8203\n",
      "Epoch 5/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 712.6653 - val_loss: 315.9279\n",
      "Epoch 6/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 554.7778 - val_loss: 272.6304\n",
      "Epoch 7/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 430.8036 - val_loss: 236.9874\n",
      "Epoch 8/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 480.9984 - val_loss: 206.3395\n",
      "Epoch 9/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 339.3443 - val_loss: 182.0294\n",
      "Epoch 10/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 355.6599 - val_loss: 160.2032\n",
      "Epoch 11/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 313.0419 - val_loss: 143.4311\n",
      "Epoch 12/75\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 235.2318 - val_loss: 126.8308\n",
      "Epoch 13/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 244.8118 - val_loss: 109.4262\n",
      "Epoch 14/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 179.3723 - val_loss: 98.5648\n",
      "Epoch 15/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 130.4217 - val_loss: 87.0086\n",
      "Epoch 16/75\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 124.0368 - val_loss: 75.9535\n",
      "Epoch 17/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 125.2445 - val_loss: 67.4029\n",
      "Epoch 18/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 102.8083 - val_loss: 59.9902\n",
      "Epoch 19/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 98.1845 - val_loss: 54.3327\n",
      "Epoch 20/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 92.1250 - val_loss: 48.5175\n",
      "Epoch 21/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 74.9382 - val_loss: 43.5572\n",
      "Epoch 22/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 66.1382 - val_loss: 39.0832\n",
      "Epoch 23/75\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 58.6132 - val_loss: 34.8968\n",
      "Epoch 24/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 55.9952 - val_loss: 31.8646\n",
      "Epoch 25/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 44.1633 - val_loss: 28.8004\n",
      "Epoch 26/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 38.6972 - val_loss: 25.9995\n",
      "Epoch 27/75\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 36.3014 - val_loss: 23.5174\n",
      "Epoch 28/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 34.4777 - val_loss: 21.4178\n",
      "Epoch 29/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 30.9754 - val_loss: 19.4106\n",
      "Epoch 30/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 30.1853 - val_loss: 17.8403\n",
      "Epoch 31/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 28.6250 - val_loss: 16.2887\n",
      "Epoch 32/75\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 25.7625 - val_loss: 15.0587\n",
      "Epoch 33/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 21.7158 - val_loss: 13.9676\n",
      "Epoch 34/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 19.8607 - val_loss: 12.8597\n",
      "Epoch 35/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 21.2916 - val_loss: 12.0028\n",
      "Epoch 36/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 15.8218 - val_loss: 11.2221\n",
      "Epoch 37/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 15.6494 - val_loss: 10.2969\n",
      "Epoch 38/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 16.2583 - val_loss: 9.5043\n",
      "Epoch 39/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 15.0362 - val_loss: 8.8971\n",
      "Epoch 40/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 13.3459 - val_loss: 8.3803\n",
      "Epoch 41/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 12.2739 - val_loss: 7.7536\n",
      "Epoch 42/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 11.1103 - val_loss: 7.1823\n",
      "Epoch 43/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 10.3347 - val_loss: 6.6720\n",
      "Epoch 44/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 12.0940 - val_loss: 6.2529\n",
      "Epoch 45/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 8.7655 - val_loss: 6.0126\n",
      "Epoch 46/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 8.1902 - val_loss: 5.6912\n",
      "Epoch 47/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 8.1559 - val_loss: 5.3274\n",
      "Epoch 48/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.2750 - val_loss: 4.9907\n",
      "Epoch 49/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.3863 - val_loss: 4.6824\n",
      "Epoch 50/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 6.6837 - val_loss: 4.4296\n",
      "Epoch 51/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 6.3831 - val_loss: 4.2170\n",
      "Epoch 52/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 6.4126 - val_loss: 4.0077\n",
      "Epoch 53/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 5.7477 - val_loss: 3.8064\n",
      "Epoch 54/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 5.1987 - val_loss: 3.6241\n",
      "Epoch 55/75\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 5.2309 - val_loss: 3.4603\n",
      "Epoch 56/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 4.8665 - val_loss: 3.3213\n",
      "Epoch 57/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 4.5608 - val_loss: 3.1806\n",
      "Epoch 58/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 4.6188 - val_loss: 3.0575\n",
      "Epoch 59/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 4.2473 - val_loss: 2.9529\n",
      "Epoch 60/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 4.1175 - val_loss: 2.8545\n",
      "Epoch 61/75\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 3.8736 - val_loss: 2.7639\n",
      "Epoch 62/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 3.7011 - val_loss: 2.6832\n",
      "Epoch 63/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.7122 - val_loss: 2.6093\n",
      "Epoch 64/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 3.6179 - val_loss: 2.5473\n",
      "Epoch 65/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 3.3755 - val_loss: 2.4862\n",
      "Epoch 66/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 3.2473 - val_loss: 2.4326\n",
      "Epoch 67/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 3.1208 - val_loss: 2.3817\n",
      "Epoch 68/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 2.9870 - val_loss: 2.3374\n",
      "Epoch 69/75\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 2.8839 - val_loss: 2.3010\n",
      "Epoch 70/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 2.7889 - val_loss: 2.2652\n",
      "Epoch 71/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 2.7290 - val_loss: 2.2328\n",
      "Epoch 72/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 2.7624 - val_loss: 2.2043\n",
      "Epoch 73/75\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 2.7125 - val_loss: 2.1822\n",
      "Epoch 74/75\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 2.6040 - val_loss: 2.1650\n",
      "Epoch 75/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 2.6834 - val_loss: 2.1505\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "c_vals [0.5172412057012531, 0.527487158087176]\n",
      "about to train <function regression_weibull at 0x7f893657a200>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in log\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/scipy/optimize/_numdiff.py:557: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in log\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to train <function regression_weibull at 0x7f893657a200>\n",
      "c_vals [0.6558127834147159, 0.6498423128329043]\n",
      "about to train <function deep_hit at 0x7f8936033830>\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 2.6521,\tval_loss: 2.6300\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 2.6418,\tval_loss: 2.6276\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 2.6218,\tval_loss: 2.6255\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 2.6345,\tval_loss: 2.6236\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 2.6292,\tval_loss: 2.6220\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 2.6109,\tval_loss: 2.6203\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 2.6167,\tval_loss: 2.6187\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 2.6060,\tval_loss: 2.6174\n",
      "8:\t[0s / 1s],\t\ttrain_loss: 2.6346,\tval_loss: 2.6161\n",
      "9:\t[0s / 1s],\t\ttrain_loss: 2.5974,\tval_loss: 2.6148\n",
      "10:\t[0s / 1s],\t\ttrain_loss: 2.6119,\tval_loss: 2.6135\n",
      "11:\t[0s / 1s],\t\ttrain_loss: 2.6016,\tval_loss: 2.6124\n",
      "12:\t[0s / 1s],\t\ttrain_loss: 2.6018,\tval_loss: 2.6113\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 2.5905,\tval_loss: 2.6103\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 2.5957,\tval_loss: 2.6094\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 2.5914,\tval_loss: 2.6085\n",
      "16:\t[0s / 2s],\t\ttrain_loss: 2.5862,\tval_loss: 2.6077\n",
      "17:\t[0s / 2s],\t\ttrain_loss: 2.5878,\tval_loss: 2.6070\n",
      "18:\t[0s / 2s],\t\ttrain_loss: 2.5943,\tval_loss: 2.6064\n",
      "19:\t[0s / 2s],\t\ttrain_loss: 2.5868,\tval_loss: 2.6058\n",
      "20:\t[0s / 2s],\t\ttrain_loss: 2.5870,\tval_loss: 2.6051\n",
      "21:\t[0s / 2s],\t\ttrain_loss: 2.5693,\tval_loss: 2.6045\n",
      "22:\t[0s / 3s],\t\ttrain_loss: 2.5765,\tval_loss: 2.6039\n",
      "23:\t[0s / 3s],\t\ttrain_loss: 2.5742,\tval_loss: 2.6035\n",
      "24:\t[0s / 3s],\t\ttrain_loss: 2.5657,\tval_loss: 2.6029\n",
      "25:\t[0s / 3s],\t\ttrain_loss: 2.5783,\tval_loss: 2.6023\n",
      "26:\t[0s / 3s],\t\ttrain_loss: 2.5775,\tval_loss: 2.6018\n",
      "27:\t[0s / 3s],\t\ttrain_loss: 2.5678,\tval_loss: 2.6014\n",
      "28:\t[0s / 3s],\t\ttrain_loss: 2.5749,\tval_loss: 2.6009\n",
      "29:\t[0s / 3s],\t\ttrain_loss: 2.5681,\tval_loss: 2.6005\n",
      "30:\t[0s / 3s],\t\ttrain_loss: 2.5712,\tval_loss: 2.6000\n",
      "31:\t[0s / 3s],\t\ttrain_loss: 2.5678,\tval_loss: 2.5996\n",
      "32:\t[0s / 4s],\t\ttrain_loss: 2.5700,\tval_loss: 2.5994\n",
      "33:\t[0s / 4s],\t\ttrain_loss: 2.5565,\tval_loss: 2.5991\n",
      "34:\t[0s / 4s],\t\ttrain_loss: 2.5552,\tval_loss: 2.5988\n",
      "35:\t[0s / 4s],\t\ttrain_loss: 2.5500,\tval_loss: 2.5985\n",
      "36:\t[0s / 4s],\t\ttrain_loss: 2.5623,\tval_loss: 2.5982\n",
      "37:\t[0s / 4s],\t\ttrain_loss: 2.5549,\tval_loss: 2.5980\n",
      "38:\t[0s / 4s],\t\ttrain_loss: 2.5552,\tval_loss: 2.5976\n",
      "39:\t[0s / 4s],\t\ttrain_loss: 2.5478,\tval_loss: 2.5974\n",
      "40:\t[0s / 4s],\t\ttrain_loss: 2.5494,\tval_loss: 2.5971\n",
      "41:\t[0s / 5s],\t\ttrain_loss: 2.5562,\tval_loss: 2.5968\n",
      "42:\t[0s / 5s],\t\ttrain_loss: 2.5628,\tval_loss: 2.5966\n",
      "43:\t[0s / 5s],\t\ttrain_loss: 2.5616,\tval_loss: 2.5963\n",
      "44:\t[0s / 5s],\t\ttrain_loss: 2.5526,\tval_loss: 2.5961\n",
      "45:\t[0s / 5s],\t\ttrain_loss: 2.5456,\tval_loss: 2.5959\n",
      "46:\t[0s / 5s],\t\ttrain_loss: 2.5492,\tval_loss: 2.5957\n",
      "47:\t[0s / 5s],\t\ttrain_loss: 2.5526,\tval_loss: 2.5955\n",
      "48:\t[0s / 6s],\t\ttrain_loss: 2.5443,\tval_loss: 2.5953\n",
      "49:\t[0s / 6s],\t\ttrain_loss: 2.5469,\tval_loss: 2.5951\n",
      "about to train <function deep_hit at 0x7f8936033830>\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 2.6775,\tval_loss: 2.6508\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 2.6807,\tval_loss: 2.6476\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 2.6957,\tval_loss: 2.6442\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 2.6678,\tval_loss: 2.6411\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 2.6750,\tval_loss: 2.6384\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 2.6598,\tval_loss: 2.6360\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 2.6640,\tval_loss: 2.6336\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 2.6504,\tval_loss: 2.6313\n",
      "8:\t[0s / 1s],\t\ttrain_loss: 2.6543,\tval_loss: 2.6293\n",
      "9:\t[0s / 1s],\t\ttrain_loss: 2.6462,\tval_loss: 2.6272\n",
      "10:\t[0s / 1s],\t\ttrain_loss: 2.6435,\tval_loss: 2.6252\n",
      "11:\t[0s / 1s],\t\ttrain_loss: 2.6463,\tval_loss: 2.6235\n",
      "12:\t[0s / 1s],\t\ttrain_loss: 2.6335,\tval_loss: 2.6218\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 2.6273,\tval_loss: 2.6203\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 2.6388,\tval_loss: 2.6189\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 2.6139,\tval_loss: 2.6175\n",
      "16:\t[0s / 1s],\t\ttrain_loss: 2.6173,\tval_loss: 2.6162\n",
      "17:\t[0s / 1s],\t\ttrain_loss: 2.6175,\tval_loss: 2.6149\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 2.6214,\tval_loss: 2.6137\n",
      "19:\t[0s / 2s],\t\ttrain_loss: 2.6098,\tval_loss: 2.6126\n",
      "20:\t[0s / 2s],\t\ttrain_loss: 2.6182,\tval_loss: 2.6116\n",
      "21:\t[0s / 2s],\t\ttrain_loss: 2.6167,\tval_loss: 2.6106\n",
      "22:\t[0s / 2s],\t\ttrain_loss: 2.6162,\tval_loss: 2.6096\n",
      "23:\t[0s / 2s],\t\ttrain_loss: 2.6039,\tval_loss: 2.6087\n",
      "24:\t[0s / 2s],\t\ttrain_loss: 2.6051,\tval_loss: 2.6076\n",
      "25:\t[0s / 2s],\t\ttrain_loss: 2.5908,\tval_loss: 2.6066\n",
      "26:\t[0s / 2s],\t\ttrain_loss: 2.6017,\tval_loss: 2.6057\n",
      "27:\t[0s / 3s],\t\ttrain_loss: 2.6008,\tval_loss: 2.6049\n",
      "28:\t[0s / 3s],\t\ttrain_loss: 2.5982,\tval_loss: 2.6042\n",
      "29:\t[0s / 3s],\t\ttrain_loss: 2.5909,\tval_loss: 2.6034\n",
      "30:\t[0s / 3s],\t\ttrain_loss: 2.5928,\tval_loss: 2.6026\n",
      "31:\t[0s / 3s],\t\ttrain_loss: 2.5853,\tval_loss: 2.6020\n",
      "32:\t[0s / 3s],\t\ttrain_loss: 2.5796,\tval_loss: 2.6014\n",
      "33:\t[0s / 3s],\t\ttrain_loss: 2.5913,\tval_loss: 2.6007\n",
      "34:\t[0s / 3s],\t\ttrain_loss: 2.5893,\tval_loss: 2.6000\n",
      "35:\t[0s / 4s],\t\ttrain_loss: 2.5809,\tval_loss: 2.5994\n",
      "36:\t[0s / 4s],\t\ttrain_loss: 2.5803,\tval_loss: 2.5989\n",
      "37:\t[0s / 4s],\t\ttrain_loss: 2.5794,\tval_loss: 2.5982\n",
      "38:\t[0s / 4s],\t\ttrain_loss: 2.5805,\tval_loss: 2.5976\n",
      "39:\t[0s / 4s],\t\ttrain_loss: 2.5714,\tval_loss: 2.5971\n",
      "40:\t[0s / 4s],\t\ttrain_loss: 2.5707,\tval_loss: 2.5965\n",
      "41:\t[0s / 4s],\t\ttrain_loss: 2.5857,\tval_loss: 2.5959\n",
      "42:\t[0s / 5s],\t\ttrain_loss: 2.5717,\tval_loss: 2.5955\n",
      "43:\t[0s / 5s],\t\ttrain_loss: 2.5713,\tval_loss: 2.5950\n",
      "44:\t[0s / 5s],\t\ttrain_loss: 2.5656,\tval_loss: 2.5945\n",
      "45:\t[0s / 5s],\t\ttrain_loss: 2.5623,\tval_loss: 2.5940\n",
      "46:\t[0s / 5s],\t\ttrain_loss: 2.5657,\tval_loss: 2.5935\n",
      "47:\t[0s / 5s],\t\ttrain_loss: 2.5718,\tval_loss: 2.5932\n",
      "48:\t[0s / 5s],\t\ttrain_loss: 2.5526,\tval_loss: 2.5928\n",
      "49:\t[0s / 5s],\t\ttrain_loss: 2.5629,\tval_loss: 2.5924\n",
      "c_vals [0.5513807094156578, 0.5885912182403561]\n",
      "about to train <function deep_weibull at 0x7f893657ad40>\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 2s 19ms/step - loss: 408.2805 - val_loss: 22.2173\n",
      "Epoch 2/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 165.2148 - val_loss: 19.5775\n",
      "Epoch 3/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 131.2511 - val_loss: 17.8900\n",
      "Epoch 4/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 139.0546 - val_loss: 15.5367\n",
      "Epoch 5/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 120.9493 - val_loss: 14.1446\n",
      "Epoch 6/75\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 87.8437 - val_loss: 12.9683\n",
      "Epoch 7/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 108.5393 - val_loss: 11.7416\n",
      "Epoch 8/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 81.9592 - val_loss: 10.6616\n",
      "Epoch 9/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 51.9815 - val_loss: 9.9043\n",
      "Epoch 10/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 47.1470 - val_loss: 8.9953\n",
      "Epoch 11/75\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 36.3044 - val_loss: 8.2342\n",
      "Epoch 12/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 48.1522 - val_loss: 7.5161\n",
      "Epoch 13/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 25.4302 - val_loss: 7.0742\n",
      "Epoch 14/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 20.4406 - val_loss: 6.6132\n",
      "Epoch 15/75\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 23.1877 - val_loss: 6.0790\n",
      "Epoch 16/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 16.6938 - val_loss: 5.6064\n",
      "Epoch 17/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 16.7056 - val_loss: 5.1982\n",
      "Epoch 18/75\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 13.3860 - val_loss: 4.8584\n",
      "Epoch 19/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 11.6791 - val_loss: 4.5428\n",
      "Epoch 20/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 12.4752 - val_loss: 4.2995\n",
      "Epoch 21/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 10.1534 - val_loss: 4.1171\n",
      "Epoch 22/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 8.5425 - val_loss: 3.9221\n",
      "Epoch 23/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 8.6942 - val_loss: 3.7276\n",
      "Epoch 24/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 8.9791 - val_loss: 3.5998\n",
      "Epoch 25/75\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 6.6684 - val_loss: 3.4793\n",
      "Epoch 26/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 6.6593 - val_loss: 3.3619\n",
      "Epoch 27/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 6.1650 - val_loss: 3.2556\n",
      "Epoch 28/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 5.8015 - val_loss: 3.1608\n",
      "Epoch 29/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 5.2215 - val_loss: 3.0726\n",
      "Epoch 30/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 4.5464 - val_loss: 3.0029\n",
      "Epoch 31/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 4.3770 - val_loss: 2.9378\n",
      "Epoch 32/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 4.3548 - val_loss: 2.8850\n",
      "Epoch 33/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4.1819 - val_loss: 2.8423\n",
      "Epoch 34/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 3.8354 - val_loss: 2.8032\n",
      "Epoch 35/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 4.0660 - val_loss: 2.7744\n",
      "Epoch 36/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.7723 - val_loss: 2.7490\n",
      "Epoch 37/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 4.3776 - val_loss: 2.7299\n",
      "Epoch 38/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.6029 - val_loss: 2.7139\n",
      "Epoch 39/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 3.4547 - val_loss: 2.7005\n",
      "Epoch 40/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.4205 - val_loss: 2.6874\n",
      "Epoch 41/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.2575 - val_loss: 2.6768\n",
      "Epoch 42/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.2886 - val_loss: 2.6702\n",
      "Epoch 43/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.8301 - val_loss: 2.6651\n",
      "Epoch 44/75\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 3.1339 - val_loss: 2.6620\n",
      "Epoch 45/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 3.1454 - val_loss: 2.6595\n",
      "Epoch 46/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 3.0396 - val_loss: 2.6574\n",
      "Epoch 47/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 3.1996 - val_loss: 2.6566\n",
      "Epoch 48/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 2.9570 - val_loss: 2.6572\n",
      "Epoch 49/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 3.1200 - val_loss: 2.6575\n",
      "Epoch 50/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.0158 - val_loss: 2.6594\n",
      "Epoch 51/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 3.0204 - val_loss: 2.6604\n",
      "Epoch 52/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 2.9437 - val_loss: 2.6617\n",
      "Epoch 53/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 2.9790 - val_loss: 2.6643\n",
      "Epoch 54/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3.0415 - val_loss: 2.6643\n",
      "Epoch 55/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 2.8977 - val_loss: 2.6645\n",
      "Epoch 56/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 2.8760 - val_loss: 2.6650\n",
      "Epoch 57/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 2.8305 - val_loss: 2.6638\n",
      "Epoch 58/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 2.8808 - val_loss: 2.6651\n",
      "Epoch 59/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.8589 - val_loss: 2.6653\n",
      "Epoch 60/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 2.8750 - val_loss: 2.6655\n",
      "Epoch 61/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 2.7978 - val_loss: 2.6621\n",
      "Epoch 62/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 2.8408 - val_loss: 2.6599\n",
      "Epoch 62: early stopping\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "about to train <function deep_weibull at 0x7f893657ad40>\n",
      "Epoch 1/75\n",
      "25/25 [==============================] - 2s 17ms/step - loss: 1590.0607 - val_loss: 415.9818\n",
      "Epoch 2/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 1340.1752 - val_loss: 364.8817\n",
      "Epoch 3/75\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 1245.8258 - val_loss: 315.7347\n",
      "Epoch 4/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 816.1677 - val_loss: 280.5457\n",
      "Epoch 5/75\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 662.7854 - val_loss: 247.3744\n",
      "Epoch 6/75\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 696.2886 - val_loss: 215.8091\n",
      "Epoch 7/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 470.5045 - val_loss: 190.6437\n",
      "Epoch 8/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 439.2036 - val_loss: 168.1021\n",
      "Epoch 9/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 325.5127 - val_loss: 147.5141\n",
      "Epoch 10/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 312.1609 - val_loss: 128.9896\n",
      "Epoch 11/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 256.9648 - val_loss: 114.2460\n",
      "Epoch 12/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 194.1478 - val_loss: 101.6068\n",
      "Epoch 13/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 207.2720 - val_loss: 89.4311\n",
      "Epoch 14/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 162.9545 - val_loss: 79.7798\n",
      "Epoch 15/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 154.3404 - val_loss: 70.1406\n",
      "Epoch 16/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 141.2680 - val_loss: 62.7499\n",
      "Epoch 17/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 121.8295 - val_loss: 56.4902\n",
      "Epoch 18/75\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 78.9288 - val_loss: 51.1027\n",
      "Epoch 19/75\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 100.6695 - val_loss: 46.3205\n",
      "Epoch 20/75\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 77.8365 - val_loss: 42.1599\n",
      "Epoch 21/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 77.3741 - val_loss: 37.8481\n",
      "Epoch 22/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 65.4326 - val_loss: 34.3040\n",
      "Epoch 23/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 55.0530 - val_loss: 31.0166\n",
      "Epoch 24/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 51.3355 - val_loss: 28.0378\n",
      "Epoch 25/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 48.5903 - val_loss: 25.4678\n",
      "Epoch 26/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 38.1604 - val_loss: 23.2012\n",
      "Epoch 27/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 39.6111 - val_loss: 21.1143\n",
      "Epoch 28/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 31.2901 - val_loss: 19.2324\n",
      "Epoch 29/75\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 31.8563 - val_loss: 17.4791\n",
      "Epoch 30/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 27.4045 - val_loss: 15.9784\n",
      "Epoch 31/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 26.7699 - val_loss: 14.6685\n",
      "Epoch 32/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 22.3979 - val_loss: 13.5614\n",
      "Epoch 33/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 21.4646 - val_loss: 12.5345\n",
      "Epoch 34/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 19.3123 - val_loss: 11.5416\n",
      "Epoch 35/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 19.2247 - val_loss: 10.7756\n",
      "Epoch 36/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 15.4267 - val_loss: 9.9809\n",
      "Epoch 37/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 16.5394 - val_loss: 9.2843\n",
      "Epoch 38/75\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 14.1858 - val_loss: 8.7623\n",
      "Epoch 39/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 13.2518 - val_loss: 8.1699\n",
      "Epoch 40/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 11.8747 - val_loss: 7.5758\n",
      "Epoch 41/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 10.7888 - val_loss: 7.0633\n",
      "Epoch 42/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 10.8850 - val_loss: 6.6160\n",
      "Epoch 43/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 9.6444 - val_loss: 6.2005\n",
      "Epoch 44/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 10.2114 - val_loss: 5.8459\n",
      "Epoch 45/75\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 7.9489 - val_loss: 5.5546\n",
      "Epoch 46/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 8.1757 - val_loss: 5.2262\n",
      "Epoch 47/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.3451 - val_loss: 4.8978\n",
      "Epoch 48/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 6.9472 - val_loss: 4.6149\n",
      "Epoch 49/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 6.8346 - val_loss: 4.3578\n",
      "Epoch 50/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 6.1969 - val_loss: 4.1211\n",
      "Epoch 51/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.9936 - val_loss: 3.9223\n",
      "Epoch 52/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 6.3858 - val_loss: 3.7564\n",
      "Epoch 53/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.1263 - val_loss: 3.6090\n",
      "Epoch 54/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 4.9288 - val_loss: 3.4522\n",
      "Epoch 55/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 5.1067 - val_loss: 3.3040\n",
      "Epoch 56/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 4.8809 - val_loss: 3.1813\n",
      "Epoch 57/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 4.4207 - val_loss: 3.0723\n",
      "Epoch 58/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.9456 - val_loss: 2.9604\n",
      "Epoch 59/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 4.0511 - val_loss: 2.8542\n",
      "Epoch 60/75\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 3.9347 - val_loss: 2.7750\n",
      "Epoch 61/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 3.7964 - val_loss: 2.6937\n",
      "Epoch 62/75\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 3.5586 - val_loss: 2.6207\n",
      "Epoch 63/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.7138 - val_loss: 2.5541\n",
      "Epoch 64/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 3.4027 - val_loss: 2.4980\n",
      "Epoch 65/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3.4596 - val_loss: 2.4446\n",
      "Epoch 66/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3.1796 - val_loss: 2.4047\n",
      "Epoch 67/75\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3.0461 - val_loss: 2.3619\n",
      "Epoch 68/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 2.9201 - val_loss: 2.3177\n",
      "Epoch 69/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 3.1007 - val_loss: 2.2783\n",
      "Epoch 70/75\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 2.8511 - val_loss: 2.2534\n",
      "Epoch 71/75\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.7439 - val_loss: 2.2267\n",
      "Epoch 72/75\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 2.6684 - val_loss: 2.2004\n",
      "Epoch 73/75\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 2.6558 - val_loss: 2.1772\n",
      "Epoch 74/75\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 2.7827 - val_loss: 2.1597\n",
      "Epoch 75/75\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.6834 - val_loss: 2.1467\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "c_vals [0.49584271329880214, 0.569859694633369]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def c_index(ev):\n",
    "\n",
    "    return ev.concordance_td('antolini')\n",
    "\n",
    "models=[\"regression_weibull\",\"deep_hit\",\"deep_weibull\"]\n",
    "#models=[\"deep_weibull\"]\n",
    "all_splits = [1,2] #[1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "for train_frac in np.arange(0.60, 0.70, 0.05).tolist():\n",
    "\n",
    "    df = metabric.read_df() # read in dataset\n",
    "    df = normalise(df, ['x0', 'x1', 'x2', 'x3', 'x8']) # normalise cols where appropriate\n",
    "    df.rename(columns={\"duration\": \"time\", \"event\": \"status\"}, inplace=True) # rename duration/event cols\n",
    "    make_train_test(df, train_frac=train_frac, dataset=\"metabric_temp\", n_splits=10) # make train/test splits\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        c_vals = [None] * len(all_splits) #10\n",
    "\n",
    "        for split in all_splits:\n",
    "\n",
    "            # run the model\n",
    "            f_model = globals()[model]\n",
    "            print(\"about to train\", f_model)\n",
    "            #gw result = f_model(\"metabric_temp\", split)\n",
    "            if model==\"deep_weibull\":\n",
    "                result = deep_weibull(\"metabric_temp\", split)\n",
    "            elif model==\"regression_weibull\":\n",
    "                result = regression_weibull(\"metabric_temp\", split)\n",
    "            elif model==\"deep_hit\":\n",
    "                result = deep_hit(\"metabric_temp\", split)\n",
    "            \n",
    "            # get evaluation object\n",
    "            ev = result[\"ev\"]\n",
    "            # compute c index\n",
    "            c_vals[split-1] = c_index(ev)\n",
    "\n",
    "        print(\"c_vals\",c_vals)\n",
    "        c = np.mean(c_vals)\n",
    "        c = round(c, ndigits=3)\n",
    "\n",
    "        new_row = {'train_frac':train_frac, 'model':model, 'mean_c':c}\n",
    "        results_df = results_df.append(new_row, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89fa4f29-dfef-454f-b4e5-95e26d62fd12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwilliams/Projects/SEEMAP2023/venv/deepweibull/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: You have mixed positional and keyword arguments, some input may be discarded.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$c$-index')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "##results_df.to_csv(\"metabric_size_results.csv\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "results_rw = results_df[results_df[\"model\"]==\"regression_weibull\"]\n",
    "results_dw = results_df[results_df[\"model\"]==\"deep_weibull\"]\n",
    "results_dh = results_df[results_df[\"model\"]==\"deep_hit\"]\n",
    "\n",
    "l1 = ax.plot(results_rw[\"train_frac\"], results_rw[\"mean_c\"], color=\"blue\")[0]\n",
    "l2 = ax.plot(results_dw[\"train_frac\"], results_dw[\"mean_c\"], color=\"orange\")[0]\n",
    "l3 = ax.plot(results_dh[\"train_frac\"], results_dh[\"mean_c\"], color=\"green\")[0]\n",
    "\n",
    "ax.legend([l1, l3, l3],     # The line objects\n",
    "#ax.legend([l2],     # The line objects\n",
    "           labels=[\"RegressionWeibull\",\"DeepWeibull\",\"DeepHit\"]  # The labels for each line\n",
    "          #labels=[\"DeepWeibull\"] \n",
    "           )\n",
    "\n",
    "plt.xlabel('Proportion of individuals in training set (%)')\n",
    "plt.ylabel(r'$c$'+\"-index\")\n",
    "\n",
    "#plot_file_path = \"plots/real_data_experiments/metabric_training_size.pdf\"\n",
    "#plt.savefig(plot_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c97c3-5993-4a79-a353-3d62ef46949d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
